{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space (BR Chapter 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector space\n",
    "\n",
    "- A **vector space** or **linear space** or **linear subspace** or **subspace** $\\mathcal{S} \\subseteq \\mathbb{R}^n$ is a set of vectors in $\\mathbb{R}^n$ that are closed under addition and scalar multiplication. In other words, $\\mathcal{S}$ must satisfy  \n",
    "    1. If $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}$, then $\\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}$.  \n",
    "    2. If $\\mathbf{x} \\in \\mathcal{S}$, then $\\alpha \\mathbf{x} \\in \\mathcal{S}$ for any $\\alpha \\in \\mathbb{R}$.    \n",
    "    Or equivalently the set is closed under _axpy_ operation. $\\alpha \\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}$ for all $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}$ and $\\alpha \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO vector diagram for addition and scalar multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any vector space must contain the zero vector $\\mathbf{0}$ (why?).\n",
    "\n",
    "- Examples of vector space:  \n",
    "    - Origin: $\\{\\mathbf{0}\\}$.  \n",
    "    - Line passing origin: $\\{\\alpha \\mathbf{x}: \\alpha \\in \\mathbb{R}\\}$ for a fixed vector $\\mathbf{x} \\in \\mathbb{R}^n$.  \n",
    "    - Plane passing origin: $\\{\\alpha_1 \\mathbf{x}_1 + \\alpha_2 \\mathbf{x}_2: \\alpha_1, \\alpha_2 \\in \\mathbb{R}\\}$ for two fixed vectors $\\mathbf{x}_1, \\mathbf{x}_2 \\in \\mathbb{R}^n$.\n",
    "    - Euclidean space: $\\mathbb{R}^n$.\n",
    "\n",
    "* Order and dimension. The **order** of a vectors space is simply the length of the vectors in that space. Not to be confused with the **dimension** of a vector space, which we later define as the maximum number of linearly independent vectors in that space.\n",
    "\n",
    "* BR Theorem 4.2. If $\\mathcal{S}_1$ and $\\mathcal{S}_2$ are two vector spaces of same order, then their **intersection** $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ is a vector space.\n",
    "\n",
    "    Proof: Let $\\mathbf{x}, \\mathbf{y}$ be two arbitrary vectors in $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ and $\\alpha \\in \\mathbb{R}$. Then $\\alpha \\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}_1$ because $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}_1$ and $\\mathcal{S}_1$ is a vector space. Similarly $\\alpha \\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}_2$. Therefore $\\alpha \\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}_1 \\cap \\mathcal{S}_2$. \n",
    "\n",
    "* Two vector spaces $\\mathcal{S}_1$ and $\\mathcal{S}_2$ are **essentially disjoint** or **virtually disjoint** if the only element in $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ is the zero vector $\\mathbf{0}$.\n",
    "\n",
    "* If $\\mathcal{S}_1$ and $\\mathcal{S}_2$ are two vector spaces of same order, then their **union** $\\mathcal{S}_1 \\cup \\mathcal{S}_2$ is not necessarily a vector space.\n",
    "\n",
    "    TODO: Give a counter example.\n",
    "\n",
    "* BR Theorem 4.3. Define the **sum** of two vector spaces $\\mathcal{S}_1$ and $\\mathcal{S}_2$ of same order by \n",
    "$$\n",
    "    \\mathcal{S}_1 + \\mathcal{S}_2 = \\{\\mathbf{x}_1 + \\mathbf{x}_2: \\mathbf{x}_1 \\in \\mathcal{S}_1, \\mathbf{x}_2 \\in \\mathcal{S}_2\\}.\n",
    "$$\n",
    "Then $\\mathcal{S}_1 + \\mathcal{S}_2$ is the smallest vector space that contains $\\mathcal{S}_1 \\cup \\mathcal{S}_2$.\n",
    "\n",
    "    Proof: Let $\\mathbf{u}, \\mathbf{v} \\in \\mathcal{S}_1 + \\mathcal{S}_2$ and $\\alpha \\in \\mathbb{R}$, we want to show $\\alpha \\mathbf{u} + \\mathbf{v} \\in \\mathcal{S}_1 + \\mathcal{S}_2$. Write $\\mathbf{u} = \\mathbf{x}_1 + \\mathbf{x}_2$ and $\\mathbf{v} = \\mathbf{y}_1 + \\mathbf{y}_2$, where $\\mathbf{x}_1, \\mathbf{y}_1 \\in \\mathcal{S}_1$ and $\\mathbf{x}_2, \\mathbf{y}_2 \\in \\mathcal{S}_2$, then\n",
    "$$\n",
    "    \\alpha \\mathbf{u} + \\mathbf{v} = (\\alpha \\mathbf{x}_1 + \\alpha \\mathbf{x}_2) + (\\mathbf{y}_1 + \\mathbf{y}_2) = (\\alpha \\mathbf{x}_1 + \\mathbf{y}_1) + (\\alpha \\mathbf{x}_2 + \\mathbf{y}_2) \\in \\mathcal{S}_1 + \\mathcal{S}_2.\n",
    "$$\n",
    "Thus $\\mathcal{S}_1 + \\mathcal{S}_2$ is a vector space.  \n",
    "To show $\\mathcal{S}_1 + \\mathcal{S}_2$ is the smallest vector space containing $\\mathcal{S}_1 \\cup \\mathcal{S}_2$, let $\\mathcal{S}_3$ be a vector space that contains $\\mathcal{S}_1 \\cup \\mathcal{S}_2$. We will show $\\mathcal{S}_3$ must contain $\\mathcal{S}_1 + \\mathcal{S}_2$. Let $\\mathbf{u} \\in \\mathcal{S}_1 + \\mathcal{S}_2$ so $\\mathbf{u} = \\mathbf{x} + \\mathbf{y}$, where $\\mathbf{x} \\in \\mathcal{S}_1$ and $\\mathbf{y} \\in \\mathcal{S}_2$. Since both $\\mathbf{x}$ and $\\mathbf{y}$ belong to $\\mathcal{S}_1 \\cup \\mathcal{S}_2 \\subseteq \\mathcal{S}_3$. So $\\mathbf{u} = \\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}_3$.\n",
    "\n",
    "* Example: Let $\\mathcal{S}_1$ and $\\mathcal{S}_2$ be two lines in $\\mathbb{R}^2$ that passes the origin and not parallel to each other. What are $\\mathcal{S}_1 \\cap \\mathcal{S}_2$ and $\\mathcal{S}_1 + \\mathcal{S}_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine space (optional)\n",
    "\n",
    "* Consider a system of $m$ linear equations in variable $\\mathbf{x} \\in \\mathbb{R}^n$\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{c}_1' \\mathbf{x} &=& b_1 \\\\\n",
    "    \\vdots &=& \\vdots \\\\\n",
    "    \\mathbf{c}_m' \\mathbf{x} &=& b_m,\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{c}_1,\\ldots,\\mathbf{c}_m \\in \\mathbb{R}^n$ are linearly independent (and hence $m \\le n$). The set of solutions is called an **affine space**. \n",
    "\n",
    "* The intersection of two affine spaces is an affine space (why?). If the zero vector $\\mathbf{0}$ belongs to the affine space, i.e., $b_1=\\cdots=b_m=0$, then it is a vector space. Thus any affine space containing the origin $\\mathbf{0}$ is a vector space, but other affine spaces are not vector spaces.\n",
    "\n",
    "* If $m=1$, the affine space is called a **hyperplane**. A hyperplane through the origin is an $(n-1)$-dimensional vector space.\n",
    "\n",
    "* If $m=n-1$, the affine space is a line. A line through the origin is a one-dimensional vector space.\n",
    "\n",
    "* The mapping $\\mathbf{x} \\mapsto \\mathbf{A} \\mathbf{x} + \\mathbf{b}$ is called an **affine function**. If $\\mathbf{b} = \\mathbf{0}$, it is called a **linear function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Span\n",
    "\n",
    "- The **span** of a set of $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n \\in \\mathbb{R}^m$, defined as the set of all possible linear combinations of $\\mathbf{x}_i$s\n",
    "$$\n",
    "    \\text{span} \\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\} = \\left\\{\\sum_{i=1}^n \\alpha_i \\mathbf{x}_i: \\alpha_i \\in \\mathbb{R} \\right\\},\n",
    "$$\n",
    "is a vector space in $\\mathbb{R}^m$.\n",
    "\n",
    "    Proof: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four fundamental subspaces\n",
    "\n",
    "Let $\\mathbf{A}$ be an $m \\times n$ matrix\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{A} = \\begin{pmatrix}\n",
    "    \\mid & & \\mid \\\\\n",
    "    \\mathbf{a}_1 & \\ldots & \\mathbf{a}_n \\\\\n",
    "    \\mid & & \\mid    \n",
    "    \\end{pmatrix}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "1. The **column space** of $\\mathbf{A}$ is\n",
    "\\begin{eqnarray*}\n",
    "    \\mathcal{C}(\\mathbf{A}) &=& \\{ \\mathbf{y} \\in \\mathbb{R}^m: \\mathbf{y} = \\mathbf{A} \\mathbf{x} \\text{ for some } \\mathbf{x} \\in \\mathbb{R}^n \\} \\\\\n",
    "    &=& \\text{span}\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}.\n",
    "\\end{eqnarray*}\n",
    "Sometimes it is also called the **image** or **range** or **manifold** of $\\mathbf{A}$.\n",
    "\n",
    "2. The **row space** of $\\mathbf{A}$ is\n",
    "\\begin{eqnarray*}\n",
    "    \\mathcal{R}(\\mathbf{A}) &=& \\mathcal{C}(\\mathbf{A}') \\\\\n",
    "    &=& \\{ \\mathbf{y} \\in \\mathbb{R}^n: \\mathbf{y} = \\mathbf{A}' \\mathbf{x} \\text{ for some } \\mathbf{x} \\in \\mathbb{R}^m \\}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "3. The **null space** or **kernel** of $\\mathbf{A}$ is\n",
    "\\begin{eqnarray*}\n",
    "    \\mathcal{N}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^n: \\mathbf{A} \\mathbf{x} = \\mathbf{0}\\}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "4. The **left null space** $\\mathbf{A}$ is\n",
    "\\begin{eqnarray*}\n",
    "    \\mathcal{N}(\\mathbf{A}') = \\{\\mathbf{x} \\in \\mathbb{R}^m: \\mathbf{A}' \\mathbf{x} = \\mathbf{0}\\}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "TODO: show these 4 sets are vector spaces.\n",
    "\n",
    "* Example: Draw the four subspaces of matrix $\\mathbf{A} = \\begin{pmatrix} 1 & -2 & -2 \\\\ 3 & -6 & -6 \\end{pmatrix}$. TODO\n",
    "\n",
    "* Example: Interpret the four subspaces of the incidence matrix of a directed graph. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effects of matrix multiplication on column/row space.**\n",
    "\n",
    "- BR Theorem 4.6. $\\mathcal{C}(\\mathbf{C}) \\subseteq \\mathcal{C}(\\mathbf{A})$ if and only if $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ for some matrix $\\mathbf{B}$.\n",
    "\n",
    "    Proof: The _if_ part is easily verified since each column of $\\mathbf{C}$ is a linear combination of columns of $\\mathbf{A}$. For the _only if_ part, assuming $\\mathcal{C}(\\mathbf{C}) \\subset \\mathcal{C}(\\mathbf{A})$, each column of $\\mathbf{C}$ is a linear combination of columns of $\\mathbf{A}$. In other words $\\mathbf{c}_j = \\mathbf{A} \\mathbf{b}_j$ for some $\\mathbf{b}_j$. Therefore $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$, where $\\mathbf{B}$ has columns $\\mathbf{b}_j$.\n",
    "    \n",
    "- BR Theorem 4.7. $\\mathcal{R}(\\mathbf{C}) \\subseteq \\mathcal{C}(\\mathbf{A})$ if and only if $\\mathbf{C} = \\mathbf{B} \\mathbf{A}$ for some matrix $\\mathbf{B}$.\n",
    "\n",
    "    Proof is similar to that for BR Theorem 4.6.\n",
    "    \n",
    "- BR Theorem 4.8. $\\mathcal{N}(\\mathbf{B}) \\subseteq \\mathcal{N}(\\mathbf{A} \\mathbf{B})$.\n",
    "\n",
    "    Proof. For any $\\mathbf{x} \\in \\mathcal{N}(\\mathbf{B})$, $\\mathbf{A} \\mathbf{B} \\mathbf{x} = \\mathbf{A} (\\mathbf{B} \\mathbf{x}) = \\mathbf{A} \\mathbf{0} = \\mathbf{0}$. Thus $\\mathbf{x} \\in \\mathcal{N}(\\mathbf{A} \\mathbf{B})$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column/row space of a partitioned matrix.**\n",
    "\n",
    "- BR Theorem 4.9. \n",
    "    \n",
    "    1. Assuming $\\mathbf{A}$ and $\\mathbf{B}$ have same number of rows, \n",
    "    $$\n",
    "    \\mathcal{C}((\\mathbf{A}, \\mathbf{B})) = \\mathcal{C}(\\mathbf{A}) + \\mathcal{C}(\\mathbf{B}).\n",
    "    $$\n",
    "    \n",
    "    2. Assuming $\\mathbf{A}$ and $\\mathbf{C}$ have same number of columns, \n",
    "$$\n",
    "    \\mathcal{R} \\left( \\begin{pmatrix} \\mathbf{A} \\\\ \\mathbf{C} \\end{pmatrix} \\right) = \\mathcal{R}(\\mathbf{A}) + \\mathcal{R}(\\mathbf{C})\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{A} \\\\ \\mathbf{C} \\end{pmatrix} \\right) = \\mathcal{N}(\\mathbf{A}) \\cap \\mathcal{N}(\\mathbf{C}).\n",
    "$$\n",
    "\n",
    "    Proof: To show part 1, we note $\\mathbf{x} \\in \\mathcal{C}((\\mathbf{A}, \\mathbf{B}))$ if and only if $\\mathbf{x} = \\mathbf{A} \\mathbf{u} + \\mathbf{B} \\mathbf{v}$ for some $\\mathbf{u}$ and $\\mathbf{v}$ if and only if $\\mathbf{x} \\in \\mathcal{C}(\\mathbf{A}) + \\mathcal{C}(\\mathbf{B})$. To show part 2, we apply part 1 to the transpose.\n",
    "    \n",
    "    Early on we showed that $\\mathcal{R}(\\mathbf{A}) + \\mathcal{R}(\\mathbf{B})$ is a vector space. Part 1 gives an alternative proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BR Theorem 4.11. **Row space and null space of a matrix are essentially disjoint.** For any matrix $\\mathbf{A}$,\n",
    "$$\n",
    "\\mathcal{R}(\\mathbf{A}) \\cap \\mathcal{N}(\\mathbf{A}) = \\{\\mathbf{0}\\}.\n",
    "$$\n",
    "\n",
    "    Proof: If $\\mathbf{x} \\in \\mathcal{R}(\\mathbf{A}) \\cap \\mathcal{N}(\\mathbf{A})$, then $\\mathbf{x} = \\mathbf{A}' \\mathbf{u}$ for some $\\mathbf{u}$ and $\\mathbf{A} \\mathbf{x} = \\mathbf{0}$. Thus $\\mathbf{x}' \\mathbf{x} = \\mathbf{u}' \\mathbf{A} \\mathbf{x} = \\mathbf{u}' \\mathbf{0} = 0$, which implies $\\mathbf{x} = \\mathbf{0}$. This shows $\\mathcal{R}(\\mathbf{A}) \\cap \\mathcal{N}(\\mathbf{A}) \\subseteq \\{\\mathbf{0}\\}$. The other direction is trivial.\n",
    "    \n",
    "- BR Theorem 4.12.\n",
    "\\begin{eqnarray*}\n",
    "    \\mathcal{N}(\\mathbf{A}'\\mathbf{A}) &=& \\mathcal{N}(\\mathbf{A}) \\\\\n",
    "    \\mathcal{N}(\\mathbf{A}\\mathbf{A}') &=& \\mathcal{N}(\\mathbf{A}').\n",
    "\\end{eqnarray*}\n",
    "\n",
    "    TODO: proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Linear independence and basis\n",
    "\n",
    "- A set of vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^m$ are **linearly dependent** if there exists coefficients $c_j$ such that $\\sum_{j=1}^n c_j \\mathbf{x}_j = \\mathbf{0}$ and $c_j$ are not all 0. \n",
    "\n",
    "    They are **linearly independent** if $\\sum_{j=1}^n c_j \\mathbf{x}_j = \\mathbf{0}$ implies $c_j=0$ for all $j$.\n",
    "\n",
    "- No linearly independent set can contain the zero vector $\\mathbf{0}$. (why?)\n",
    "\n",
    "- A set of linearly independent vectors that generate or span a vector space $\\mathcal{S}$ is called a **basis** of ${\\cal S}$.\n",
    "\n",
    "- BR Lemma 4.5. Let $\\mathcal{A} = \\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\}$ be a basis of a vector space $\\mathcal{S}$. Then any vector $\\mathbf{x} \\in \\mathcal{S}$ can be expressed **uniquely** as a linear combination of vectors in $\\mathcal{A}$.\n",
    "\n",
    "    Proof: Suppose $\\mathbf{x}$ can be expressed by two linear combinations\n",
    "    $$\n",
    "        \\mathbf{x} = \\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_n \\mathbf{a}_n = \\beta_1 \\mathbf{a}_1 + \\cdots + \\beta_n \\mathbf{a}_n.\n",
    "    $$\n",
    "    Then $(\\alpha_1 - \\beta_1) \\mathbf{a}_1 + \\cdots (\\alpha_n - \\beta_n) \\mathbf{a}_n = \\mathbf{0}$. Since $\\mathbf{a}_i$ are linearly independent, we have $\\alpha_i = \\beta_i$ for all $i$.\n",
    "\n",
    "- If the vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^m$ are linearly independent, then $m \\ge n$.\n",
    "\n",
    "- If $\\mathcal{A}=\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_k\\}$ is a linearly independent set in a vector space $\\mathcal{S} \\subseteq \\mathbb{R}^{m}$ and $\\mathcal{B}=\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_l\\}$ spans $\\mathcal{S}$, then $k \\le l$.\n",
    "\n",
    "    Proof. Define two matrices\n",
    "$$\n",
    "    \\mathbf{A} = \\begin{pmatrix}\n",
    "    \\mid & & \\mid \\\\\n",
    "    \\mathbf{a}_1 & \\ldots & \\mathbf{a}_k \\\\\n",
    "    \\mid & & \\mid    \n",
    "    \\end{pmatrix} \\in \\mathbb{R}^{m \\times k}, \\quad \\mathbf{B} = \\begin{pmatrix}\n",
    "    \\mid & & \\mid \\\\\n",
    "    \\mathbf{b}_1 & \\ldots & \\mathbf{b}_l \\\\\n",
    "    \\mid & & \\mid    \n",
    "    \\end{pmatrix} \\in \\mathbb{R}^{m \\times l}.\n",
    "$$\n",
    "Since $\\mathcal{B}$ spans $\\mathcal{V}$, $\\mathbf{a}_i = \\mathbf{B} \\mathbf{c}_i$ for some vector $\\mathbf{c}_i \\in \\mathbb{R}^k$ for $i=1,\\ldots,k$. Let\n",
    "$$\n",
    "    \\mathbf{C} = \\begin{pmatrix}\n",
    "    \\mid & & \\mid \\\\\n",
    "    \\mathbf{c}_1 & \\ldots & \\mathbf{c}_l \\\\\n",
    "    \\mid & & \\mid    \n",
    "    \\end{pmatrix} \\in \\mathbb{R}^{l \\times k}.\n",
    "$$\n",
    "Then $\\mathbf{A} = \\mathbf{B} \\mathbf{C}$. Since\n",
    "$$\n",
    "    \\mathcal{N}(\\mathbf{C}) \\subseteq \\mathcal{N}(\\mathbf{A}) = \\{\\mathbf{0}\\}.\n",
    "$$\n",
    "The only solution to $\\mathbf{C} \\mathbf{x} = \\mathbf{0}_k$ is $\\mathbf{0}_l$. In other words, the columns of $\\mathbf{C}$ are linearly independent. Thus $\\mathbf{C}$ has at least as many rows as columns. That is $k \\le l$.\n",
    "\n",
    "- BR Theorem 4.20. Let $\\mathcal{A}=\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_k\\}$ and $\\mathcal{B}=\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_l\\}$ be two basis of a vector space $\\mathcal{S}$. Then $k = l$.\n",
    "\n",
    "    Proof: This immediately follows from the previous result.\n",
    "    \n",
    "- The **dimension** of a vector space $\\mathcal{S}$, denoted by $\\text{dim}(\\mathcal{S})$, is defined as the number of vectors in _any_ basis of $\\mathcal{S}$, or equivalently the maximmal number of linearly independent vectors in $\\mathcal{S}$.\n",
    "\n",
    "- **Monotonicity of dimension**. If $\\mathcal{S}_1 \\subseteq \\mathcal{S}_2$ are two vector spaces, then $\\text{dim}(\\mathcal{S}_1) \\le \\text{dim}(\\mathcal{S}_2)$.\n",
    "\n",
    "    Proof: Any independent set of vectors in $\\mathcal{S}_1$ also live in $\\mathcal{S}_2$. Thus the maximal number of independent vectors in $\\mathcal{S}_2$ can only be larger or equal to the maximal number of indepedent vectors in $\\mathcal{S}_1$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
