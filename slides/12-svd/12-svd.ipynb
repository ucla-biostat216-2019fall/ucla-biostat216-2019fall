{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "- We saw in earlier lectures that a matrix can have complex eigenvalues and eigenvectors. Symmetric matrices have the nice property that all eigenvalues and eigenvectors are real. The singular value decomposition (SVD) generalizes the spectral decomposition to general rectangular matrices.\n",
    "\n",
    "- Statisticians call SVD the singly most valuable decomposition. \n",
    "\n",
    "- Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ with $\\text{rank}(\\mathbf{A})=r$. We assume $m \\ge n$. Instead of the eigen-equation, now we have\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{A} \\mathbf{v}_i &=& \\sigma_i \\mathbf{u}_i, \\quad i = 1,\\ldots,r \\\\\n",
    "\\mathbf{A} \\mathbf{v}_i &=& 0 \\, \\mathbf{u}_i, \\quad i = r+1,\\ldots,n,\n",
    "\\end{eqnarray*}\n",
    "where the **left singular vectors** $\\mathbf{u}_i \\in \\mathbb{R}^m$ are orthonormal, the **right singular vectors** $\\mathbf{v}_i \\in \\mathbb{R}^n$ are orthonormal, and the **singular values** \n",
    "$$\n",
    "\\sigma_1 \\ge \\cdots \\ge \\sigma_r > \\sigma_{r+1} = \\cdots = \\sigma_{n} = 0.\n",
    "$$\n",
    "\n",
    "- Collecting above equations into matrix multiplication format:\n",
    "$$\n",
    "\\mathbf{A} \\begin{pmatrix} \\mid & & \\mid \\\\ \\mathbf{v}_1 & \\cdots & \\mathbf{v}_n \\\\ \\mid & & \\mid \\end{pmatrix} = \\begin{pmatrix} \\mid & & \\mid \\\\ \\mathbf{u}_1 & \\cdots & \\mathbf{u}_n \\\\ \\mid & & \\mid \\end{pmatrix} \\begin{pmatrix} \\sigma_1 & & & \\\\ & \\ddots & & \\\\ & & \\sigma_r & \\\\ & & & \\mathbf{O}_{n-r} \\end{pmatrix},\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{V} = \\mathbf{U} \\boldsymbol{\\Sigma}.\n",
    "$$\n",
    "Multiplying both sides by $\\mathbf{V}'$, we have the famous **singular value decomposition (SVD)**\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}' = \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1' + \\cdots + \\sigma_r \\mathbf{u}_r \\mathbf{v}_r'.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 3.0  0.0\n",
       " 4.0  5.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "A = [3.0 0.0; 4.0 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64,Float64,Array{Float64,2},Array{Float64,1}}\n",
       "eigenvalues:\n",
       "2-element Array{Float64,1}:\n",
       " 3.0\n",
       " 5.0\n",
       "eigenvectors:\n",
       "2×2 Array{Float64,2}:\n",
       "  0.447214  0.0\n",
       " -0.894427  1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVD{Float64,Float64,Array{Float64,2}}([-0.316227766016838 -0.9486832980505135; -0.9486832980505135 0.3162277660168379], [6.70820393249937, 2.2360679774997894], [-0.7071067811865475 -0.7071067811865475; -0.7071067811865475 0.7071067811865475])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reduced form of the SVD.** If we just keep the first $r$ singular triplets ($\\sigma_i, \\mathbf{u}_i, \\mathbf{v}_i$), then\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U}_r \\boldsymbol{\\Sigma}_r \\mathbf{V}_r',\n",
    "$$\n",
    "where $\\mathbf{U}_r \\in \\mathbb{R}^{m \\times r}$, $\\boldsymbol{\\Sigma}_r = \\text{diag}(\\sigma_1, \\ldots, \\sigma_r)$, and $\\mathbf{V}_r \\in \\mathbb{R}^{n \\times r}$. \n",
    "\n",
    "- **Full SVD.** Opposite to the reduced format of SVD, we can also augment the $\\mathbf{U}$ matrix to be a square orthogonal matrix to obtain the **full SVD**\n",
    "$$\n",
    "\\mathbf{A} = \\begin{pmatrix} \\mid & & \\mid \\\\ \\mathbf{u}_1 & \\cdots & \\mathbf{u}_m \\\\ \\mid & & \\mid \\end{pmatrix} \\begin{pmatrix} \\sigma_1 & & & \\\\ & \\ddots & & \\\\ & & \\sigma_r & \\\\ & & & \\mathbf{O}_{n-r} \\\\ & \\mathbf{O}_{m-n, n} \\end{pmatrix} \\begin{pmatrix} - & \\mathbf{v}_1' & - \\\\\n",
    "& \\vdots & \\\\ - & \\mathbf{v}_n' & - \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD tells everything about $\\mathbf{A}$\n",
    "\n",
    "<img src=\"../02-vecsp/four_fundamental_subspaces.png\" width=400 align=\"center\"/>\n",
    "\n",
    "- **SVD and four fundamental subspaces.** Given the full SVD\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{A} &=& \\begin{pmatrix} \\mid & & \\mid & \\mid & & \\mid \\\\ \\mathbf{u}_1 & \\cdots & \\mathbf{u}_r & \\mathbf{u}_{r+1} & \\cdots & \\mathbf{u}_n \\\\ \\mid & & \\mid & \\mid & & \\mid \\end{pmatrix} \\begin{pmatrix} \\sigma_1 & & & \\\\ & \\ddots & & \\\\ & & \\sigma_r & \\\\ & & & \\mathbf{O}_{n-r}  \\\\ & & \\mathbf{O}_{m-n, n} & \\end{pmatrix} \\begin{pmatrix} - & \\mathbf{v}_1' & - \\\\ & \\vdots & \\\\ - & \\mathbf{v}_r' & - \\\\ - & \\mathbf{v}_{r+1}' & - \\\\ & \\vdots & \\\\ - & \\mathbf{v}_n' & - \\end{pmatrix} \\\\\n",
    "&=& \\begin{pmatrix} \\mathbf{U}_r & \\mathbf{U}_{m-r} \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{\\Sigma}_r & & \\\\ & & \\mathbf{O}_{n-r} \\\\ & \\mathbf{O}_{m-n,n} & \\end{pmatrix} \\begin{pmatrix} \\mathbf{V}_r' \\\\ \\mathbf{V}_{n-r}' \\end{pmatrix},\n",
    "\\end{eqnarray*}\n",
    "Then\n",
    "    1. $\\mathcal{C}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{U}_r)$;  $\\quad \\mathbf{U}_r \\mathbf{U}_r'$ is the orthogonal projector into $\\mathcal{C}(\\mathbf{A})$.\n",
    "    2. $\\mathcal{N}(\\mathbf{A}') = \\mathcal{C}(\\mathbf{U}_{m-r})$; $\\quad \\mathbf{U}_{m-r} \\mathbf{U}_{m-r}'$ is the orthogonal projector into $\\mathcal{N}(\\mathbf{A}')$.\n",
    "    3. $\\mathcal{C}(\\mathbf{A}') = \\mathcal{C}(\\mathbf{V}_{n-r})$; $\\quad \\mathbf{V}_{n-r} \\mathbf{V}_{n-r}'$ is the orthogonal projector into $\\mathcal{C}(\\mathbf{A}')$.\n",
    "    4. $\\mathcal{N}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{V}_r)$; $\\quad \\mathbf{V}_r \\mathbf{V}_r'$ is the orthogonal projector into $\\mathcal{N}(\\mathbf{A})$.\n",
    "    \n",
    "- $\\text{rank}(\\mathbf{A}) = r = \\text{# positive singular values}$.  \n",
    "\n",
    "- $\\|\\mathbf{A}\\|_{\\text{F}}^2 = \\sum_{i,j} a_{ij}^2 = \\sum_i \\sigma_i^2$.\n",
    "\n",
    "- Spectral norm: $\\|\\mathbf{A}\\|_2 = \\sigma_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of the SVD using eigen-decomposition\n",
    "\n",
    "- From SVD $\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}'$, we have\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{A}' \\mathbf{A} &=& (\\mathbf{V} \\boldsymbol{\\Sigma} \\mathbf{U}') (\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}') = \\mathbf{V} \\boldsymbol{\\Sigma}^2 \\mathbf{V}' \\\\\n",
    "    \\mathbf{A} \\mathbf{A}' &=& (\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}') (\\mathbf{V} \\boldsymbol{\\Sigma} \\mathbf{U}') = \\mathbf{U} \\boldsymbol{\\Sigma}^2 \\mathbf{U}'.\n",
    "\\end{eqnarray*}\n",
    "This says \n",
    "    - $\\mathbf{u}_i$ are simply the eigenvectors of the symmetric matrix $\\mathbf{A} \\mathbf{A}'$\n",
    "    - $\\mathbf{v}_i$ are simply the eigenvectors of the symmetric matrix $\\mathbf{A} \\mathbf{A}'$\n",
    "    - $\\sigma_i$ are simply $\\sqrt{\\lambda_i}$.\n",
    "    \n",
    "    Proof of SVD: Start from positive eigenvalues $\\lambda_i > 0$ and corresponding (orthonormal) eigenvectors $\\mathbf{v}_i$ of $\\mathbf{A} \\mathbf{A}'$. Set $\\sigma_i = \\sqrt \\lambda_i$ and\n",
    "$$\n",
    "\\mathbf{u}_i = \\frac{\\mathbf{A} \\mathbf{v}_i}{\\sigma_i}, \\quad i = 1,\\ldots,r.\n",
    "$$\n",
    "Verify that $\\mathbf{u}_i$ are orthonormal. Lastly, augment $\\mathbf{u}_i$s by an orthogonal basis in $\\mathcal{N}(\\mathbf{A}')$ and augment $\\mathbf{v}_i$s by an orthogonal basis in $\\mathcal{N}(\\mathbf{A})$.\n",
    "\n",
    "- Another relation of SVD to eigen-decomposition:\n",
    "$$\n",
    "\\begin{pmatrix} \\mathbf{O}_n & \\mathbf{A}' \\\\ \\mathbf{A} & \\mathbf{O}_m \\end{pmatrix} = \\frac{1}{\\sqrt 2} \\begin{pmatrix} \\mathbf{V} & \\mathbf{V} \\\\ \\mathbf{U} & - \\mathbf{U} \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{\\Sigma} & \\mathbf{O}_n \\\\ \\mathbf{O}_n & - \\boldsymbol{\\Sigma} \\end{pmatrix} \\frac{1}{\\sqrt 2} \\begin{pmatrix} \\mathbf{V}' & \\mathbf{U}' \\\\ \\mathbf{V}' & - \\mathbf{U}' \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "- TODO: example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question: If $\\mathbf{A} = \\mathbf{Q} \\boldsymbol{\\Lambda} \\mathbf{Q}'$ is symmetric positive definite, what is its SVD?\n",
    "\n",
    "    Answer: The SVD is exactly same as eigen-decomposition $\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}' = \\mathbf{Q} \\boldsymbol{\\Lambda} \\mathbf{Q}'$.\n",
    "    \n",
    "    TODO: example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question: If $\\mathbf{A} = \\mathbf{Q} \\boldsymbol{\\Lambda} \\mathbf{Q}'$ is symmetric and has negative eigenvalues, then what is its SVD?\n",
    "\n",
    "    Answer: Its SVD is\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q} \\boldsymbol{\\Lambda} \\mathbf{Q}' = \\mathbf{Q} \\begin{pmatrix} |\\lambda_1| & & \\\\ & \\ddots & \\\\ & & |\\lambda_n| \\end{pmatrix} \\begin{pmatrix} \\text{sgn}(\\lambda_1) & & \\\\ & \\ddots & \\\\ & & \\text{sgn}(\\lambda_n) \\end{pmatrix} \\mathbf{Q}'.\n",
    "$$\n",
    "\n",
    "    TODO: example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question: Why the singular values of an orthogonal matrix $\\mathbf{Q}$ are all 1?\n",
    "\n",
    "    Answer: $\\mathbf{Q}' \\mathbf{Q} = \\mathbf{Q} \\mathbf{Q}' = \\mathbf{I}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why are all eigenvalues of a square matrix less than or equal to $\\sigma_1$? \n",
    "\n",
    "    Answer: Orthogonal matrices preserve vector length\n",
    "$$\n",
    "\\|\\mathbf{A} \\mathbf{x}\\| = \\|\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}' \\mathbf{x}\\| = \\|\\boldsymbol{\\Sigma} \\mathbf{V}' \\mathbf{x}\\| \\le \\sigma_1 \\|\\mathbf{V}' \\mathbf{x}\\| = \\sigma_1 \\|\\mathbf{x}\\|.\n",
    "$$\n",
    "Now an eigenvector $\\mathbf{x}$ satisifies\n",
    "$$\n",
    "\\|\\mathbf{A} \\mathbf{x}\\| = |\\lambda| \\|\\mathbf{x}\\|.\n",
    "$$\n",
    "Thus we have $|\\lambda| \\le \\sigma_1$.\n",
    "\n",
    "    TODO: example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\mathbf{A} = \\mathbf{x} \\mathbf{y}'$, find the singular values and singular vectors. Check $|\\lambda| \\le \\sigma_1$.\n",
    "\n",
    "    Answer: TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry of SVD\n",
    "\n",
    "Visualize how a matrix acts on a vector via SVD:\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{x} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}' \\mathbf{x}.\n",
    "$$\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular vectors and Rayleigh quotient\n",
    "\n",
    "- Goal: Maximize the **Rayleigh quotient**\n",
    "$$\n",
    "\\text{maximize} \\quad f(\\mathbf{x}) = \\frac{\\|\\mathbf{A} \\mathbf{x}\\|^2}{\\|\\mathbf{x}\\|^2} = \\frac{\\mathbf{x}' \\mathbf{A}' \\mathbf{A} \\mathbf{x}}{\\mathbf{x}' \\mathbf{x}} = \\frac{\\mathbf{x}' \\mathbf{S} \\mathbf{x}}{\\mathbf{x}' \\mathbf{x}}.\n",
    "$$\n",
    "\n",
    "- Let's calculate the partial derivatives of the objective function $f(\\mathbf{x})$\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_i} = \\frac{\\left(2\\sum_j s_{ij} x_j\\right) (\\mathbf{x}' \\mathbf{x}) - (\\mathbf{x}' \\mathbf{S} \\mathbf{x}) (2x_i)}{(\\mathbf{x}' \\mathbf{x})^2} = 2 (\\mathbf{x}' \\mathbf{x})^{-1} \\left(\\sum_j s_{ij} x_j - f(\\mathbf{x}) x_i \\right).\n",
    "$$\n",
    "Collecting partial derivatives into the gradient vector and setting it to zero\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = 2 (\\mathbf{x}' \\mathbf{x})^{-1} \\left[ \\mathbf{S} \\mathbf{x} - f(\\mathbf{x}) \\cdot \\mathbf{x} \\right] = \\mathbf{0}\n",
    "$$\n",
    "yields\n",
    "$$\n",
    "\\mathbf{S} \\mathbf{x} = f(\\mathbf{x}) \\cdot \\mathbf{x}.\n",
    "$$\n",
    "That is the optimal $\\mathbf{x}$ must be an eigenvector of $\\mathbf{S} = \\mathbf{A}' \\mathbf{A}$ with corresponding eigenvalue $f(\\mathbf{x})$. Early on we also showed that\n",
    "$$\n",
    "\\|\\mathbf{A} \\mathbf{x}\\| \\le \\sigma_1 \\|\\mathbf{x}\\|\n",
    "$$\n",
    "for any $\\mathbf{x}$. Thus the global maximum is given by the top right singular vector $\\mathbf{x}^\\star = \\mathbf{v}_1$ with the optimal value equal to $\\lambda_1 = \\sigma_1^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarly, the second right singular vector is the solution to the optimization problem\n",
    "\\begin{eqnarray*}\n",
    "\\text{maximize} &\\quad& f(\\mathbf{x}) = \\frac{\\|\\mathbf{A} \\mathbf{x}\\|^2}{\\|\\mathbf{x}\\|^2} = \\frac{\\mathbf{x}' \\mathbf{A}' \\mathbf{A} \\mathbf{x}}{\\mathbf{x}' \\mathbf{x}} = \\frac{\\mathbf{x}' \\mathbf{S} \\mathbf{x}}{\\mathbf{x}' \\mathbf{x}} \\\\\n",
    "\\text{subject to} &\\quad& \\mathbf{x} \\perp \\mathbf{v}_1.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "    The proof uses the method of Lagrange multipliers.\n",
    "    \n",
    "- Theorem. **Submatrices have smaller singular values**. Let $\\mathbf{B}$ be a submatrix of $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. Then\n",
    "$$\n",
    "\\|\\mathbf{B}\\| \\le \\|\\mathbf{A}\\|\n",
    "$$\n",
    "or equivalently\n",
    "$$\n",
    "\\sigma_1 (\\mathbf{B}) \\le \\sigma_1 (\\mathbf{A}).\n",
    "$$\n",
    "\n",
    "    Proof: Let $\\tilde{\\mathbf{y}} \\in \\mathbb{R}^n$ hold corresponding entries in $\\mathbf{y}$ and be zero elsewhere. Then\n",
    "$$\n",
    "\\sigma_1 (\\mathbf{B}) = \\max \\frac{\\|\\mathbf{B} \\mathbf{y}\\|}{\\|\\mathbf{y}\\|} = \\frac{\\|\\mathbf{A} \\tilde{\\mathbf{y}}\\|}{\\|\\tilde{\\mathbf{y}}\\|} \\le \\max \\frac{\\|\\mathbf{A} \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} = \\sigma_1 (\\mathbf{A}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eckart-Young theorem: best approximation to $\\mathbf{A}$\n",
    "\n",
    "- **TODO**: Golub picture example.\n",
    "\n",
    "- SVD has some inherent optimality properties. The prescribe how to approximate a general matrix $\\mathbf{A}$ by a low rank matrix.\n",
    "\n",
    "- Before talking about approximation, we need metric that measures the quality of approximation. We discuss 3 matrix norms.\n",
    "\n",
    "- **Spectral norm** or $\\ell_2$ norm:\n",
    "$$\n",
    "\\|\\mathbf{A}\\|_2 = \\max \\frac{\\|\\mathbf{A} \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} = \\sigma_1.\n",
    "$$\n",
    "\n",
    "- **Frobeniusnorm**:\n",
    "$$\n",
    "\\|\\mathbf{A}\\|_{\\text{F}} = \\sqrt{\\sum_{i,j} a_{ij}^2} = \\text{tr}(\\mathbf{A}' \\mathbf{A}) = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2}.\n",
    "$$\n",
    "\n",
    "- **Nuclear norm**:\n",
    "$$\n",
    "\\|\\mathbf{A}\\|_{\\text{nuc}} = \\sigma_1 + \\cdots + \\sigma_r.\n",
    "$$\n",
    "\n",
    "- These 3 norms already have different values for the identity matrix:\n",
    "\\begin{eqnarray*}\n",
    "    \\|\\mathbf{I}_n\\|_2 &=& 1 \\\\\n",
    "    \\|\\mathbf{I}_n\\|_{\\text{F}} &=& \\sqrt{n} \\\\\n",
    "    \\|\\mathbf{I}_n\\|_{\\text{nuc}} &=& n.\n",
    "\\end{eqnarray*}\n",
    "Indeed for any orthogonal matrix $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$,\n",
    "\\begin{eqnarray*}\n",
    "    \\|\\mathbf{Q}\\|_2 &=& 1 \\\\\n",
    "    \\|\\mathbf{Q}\\|_{\\text{F}} &=& \\sqrt{n} \\\\\n",
    "    \\|\\mathbf{Q}\\|_{\\text{nuc}} &=& n.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- **Invariance under orthogonal transform.** For all three norms,\n",
    "$$\n",
    "\\|\\mathbf{Q}_1 \\mathbf{A} \\mathbf{Q}_2'\\| = \\mathbf{A} \\text{ for orthogonal } \\mathbf{Q}_1 \\text{ and } \\mathbf{Q}_2.\n",
    "$$\n",
    "\n",
    "- **Eckart-Young theorem.** Assuming SVD of $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ with rank $r$:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}' = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i'.\n",
    "$$\n",
    "Then the rank-$k$ matrix\n",
    "$$\n",
    "\\mathbf{A}_k = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i'\n",
    "$$\n",
    "is the best approximation to the original matrix $\\mathbf{A}$ in the 3 matrix norms ($\\ell_2$, Frobenius, and nuclear). That is\n",
    "$$\n",
    "\\|\\mathbf{A} - \\mathbf{B}\\|\n",
    "$$\n",
    "is minimized by $\\mathbf{A}_k$ among all matrices with rank $\\le r$.\n",
    "\n",
    "- **Proof of Eckart-Young theorem for the $\\ell_2$ norm.** \n",
    "\n",
    "    First we note\n",
    "$$\n",
    "\\|\\mathbf{A} - \\mathbf{A}_k\\| = \\left\\|\\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i' - \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i' \\right\\| = \\left\\|\\sum_{i=k+1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i' \\right\\| = \\sigma_{k+1}.\n",
    "$$\n",
    "We want to show that\n",
    "$$\n",
    "\\|\\mathbf{A} - \\mathbf{B}\\| = \\max \\frac{\\|(\\mathbf{A} - \\mathbf{B}) \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\ge \\sigma_{k+1}\n",
    "$$\n",
    "for any $\\mathbf{B}$ with $\\text{rank}(\\mathbf{B}) \\le k$. We show this by choosing a particular $\\mathbf{x}$ such that \n",
    "$$\n",
    "\\frac{\\|(\\mathbf{A} - \\mathbf{B}) \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\ge \\sigma_{k+1}.\n",
    "$$\n",
    "Choose $\\mathbf{x} \\ne \\mathbf{0}$ such that \n",
    "$$\n",
    "\\mathbf{B} \\mathbf{x} = \\mathbf{0} \\text{ and } \\mathbf{x} = \\sum_{i=1}^{k+1} c_i \\mathbf{v}_i.\n",
    "$$\n",
    "There exists such $\\mathbf{x}$ because (1) $\\mathcal{N}(\\mathbf{B})$ has dimension $\\ge n-k$ because $\\text{rank}(\\mathbf{B}) \\le k$ (rank-nullity theorem) and (2) $\\mathbf{v}_1, \\ldots, \\mathbf{v}_{k+1}$ span a subspace of dimension $k+1$. Thus $\\mathcal{N}(\\mathbf{B})$ and $\\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{k+1}\\}$ has a non-trivial intersection. For this $\\mathbf{x}$, we have\n",
    "\\begin{eqnarray*}\n",
    "& & \\|(\\mathbf{A} - \\mathbf{B}) \\mathbf{x}\\|^2 \\\\\n",
    "&=& \\|\\mathbf{A} \\mathbf{x}\\|^2 \\\\\n",
    "&=& \\left\\|\\left(\\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i'\\right)\\left(\\sum_{i=1}^{k+1} c_i \\mathbf{v}_i\\right)\\right\\|^2 \\\\\n",
    "&=& \\left\\| \\sum_{i=1}^{k+1} c_i \\sigma_i \\mathbf{u}_i \\right\\|^2 \\\\\n",
    "&=& \\sum_{i=1}^{k+1} c_i^2 \\sigma_i^2 \\\\\n",
    "&\\ge& \\left(\\sum_{i=1}^{k+1} c_i^2\\right) \\sigma_{k+1}^2 \\\\\n",
    "&=& \\|\\mathbf{x}\\|^2 \\sigma_{k+1}^2.\n",
    "\\end{eqnarray*}\n",
    "The proof is finished.\n",
    "\n",
    "- **(Nathan Srebro's) Proof of Eckart-Young theorem for the Frobenius norm.** \n",
    "\n",
    "    Let $\\mathbf{B}$ be a matrix of rank $\\le k$. By the rank factorization, $\\mathbf{B} = \\mathbf{C} \\mathbf{R}$, where $\\mathbf{C} \\in \\mathbb{R}^{m \\times k}$ and $\\mathbf{R} \\in \\mathbb{R}^{k \\times n}$. Using SVD $\\mathbf{B} = \\mathbf{U}_k \\boldsymbol{\\Sigma}_k \\mathbf{V}_k'$, we can take $\\mathbf{C} = \\mathbf{U}_k \\boldsymbol{\\Sigma}_k$ and $\\mathbf{R} = \\mathbf{V}_k'$. Thus $\\mathbf{C}$ has orthogonal columns and $\\mathbf{R}$ has orthonormal rows. In order for $\\mathbf{B} = \\mathbf{C} \\mathbf{R}$ to minimize\n",
    "$$\n",
    "f(\\mathbf{C}, \\mathbf{R}) = \\|\\mathbf{A} - \\mathbf{C} \\mathbf{R}\\|_{\\text{F}}^2,\n",
    "$$\n",
    "the partial derivatives must vanish\n",
    "\\begin{eqnarray*}\n",
    "    \\frac{\\partial f(\\mathbf{C}, \\mathbf{R})}{\\partial \\mathbf{C}} &=& - 2(\\mathbf{A} - \\mathbf{C} \\mathbf{R}) \\mathbf{R}' = \\mathbf{O} \\\\\n",
    "\\frac{\\partial f(\\mathbf{C}, \\mathbf{R})}{\\partial \\mathbf{R}} &=& - 2(\\mathbf{A}' - \\mathbf{R}' \\mathbf{C}') \\mathbf{C} = \\mathbf{O}.\n",
    "\\end{eqnarray*}\n",
    "The first equation shows\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{R}' = \\mathbf{C} \\mathbf{R} \\mathbf{R}' = \\mathbf{C}.\n",
    "$$    \n",
    "Then by the second equation\n",
    "$$\n",
    "\\mathbf{A}' \\mathbf{A} \\mathbf{R}' = \\mathbf{A}' \\mathbf{C} =  \\mathbf{R}' \\mathbf{C}' \\mathbf{C} = \\mathbf{R}' \\mathbf{D}.\n",
    "$$\n",
    "This says the columns of $\\mathbf{R}'$ (rows of $\\mathbf{R}$) must be eigenvectors of $\\mathbf{A}' \\mathbf{A}$, or equivalently, right singular vectors $\\mathbf{v}_i$ of $\\mathbf{A}$. Similarly the columns of $\\mathbf{C}$ must be eigenvectors of $\\mathbf{A} \\mathbf{A}'$, or equivalently, left singular vectors $\\mathbf{u}_i$ of $\\mathbf{A}$:\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{A}' \\mathbf{C} = \\mathbf{A} \\mathbf{R}' \\mathbf{D} = \\mathbf{C} \\mathbf{D}.\n",
    "$$\n",
    "Which $\\mathbf{u}_i$ and $\\mathbf{v}_i$ shall we take to minimize $f$? Apparently we should choose those with largest singular values to achieve the minimum value $\\sigma_{k+1}^2 + \\cdots + \\sigma_r^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "\n",
    "- PCA is a dimension reduction technique that finds the most informative linear combinations of the $p$ random variables $\\mathbf{X} \\in \\mathbb{R}^p$. Mathematically it finds the linear combinations of the $p$ variables that have the largest variances. If we know the population covariance of the $p$ variables is $\\text{Cov}(\\mathbf{X}) = \\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$, then\n",
    "$$\n",
    "\\text{Var}(\\mathbf{a}' \\mathbf{X}) = \\mathbf{a}' \\text{Cov}(\\mathbf{X}) \\mathbf{a} = \\mathbf{a}' \\boldsymbol{\\Sigma} \\mathbf{a}.\n",
    "$$\n",
    "Apparently the larger magnitude of $\\mathbf{a}$, the large variance $\\text{Var}(\\mathbf{a}' \\mathbf{X})$. It makes sense to maximize the normalized version\n",
    "$$\n",
    "\\text{maximize} \\quad \\frac{\\mathbf{a}' \\boldsymbol{\\Sigma} \\mathbf{a}}{\\mathbf{a}' \\mathbf{a}}.\n",
    "$$\n",
    "\n",
    "- Given a data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, where there are $n$ observations of $p$ variables. We would substitute in the sample covariance matrix\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\Sigma}} = \\frac{\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}}{n-1}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\text{maximize} \\quad \\frac{\\mathbf{a}' \\widehat{\\boldsymbol{\\Sigma}} \\mathbf{a}}{\\mathbf{a}' \\mathbf{a}}.\n",
    "$$\n",
    "From earlier discussion we know the optimal $\\mathbf{a}^\\star$ maximizing this Rayleigh quotient is the top eigenvector of $\\widehat{\\boldsymbol{\\Sigma}}$, or equivalently, the top right singular vector $\\mathbf{v}_1$ of $\\tilde{\\mathbf{X}}$, achieving the optimal value $\\lambda_1$.\n",
    "\n",
    "    Similarly, right singular vectors $\\mathbf{v}_k$ maximizes\n",
    "$$\n",
    "\\frac{\\mathbf{a}' \\widehat{\\boldsymbol{\\Sigma}} \\mathbf{a}}{\\mathbf{a}' \\mathbf{a}}\n",
    "$$\n",
    "subject to the constraint $\\mathbf{a} \\perp \\mathbf{v}_i$ for $i =1,\\ldots,k-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genome example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
